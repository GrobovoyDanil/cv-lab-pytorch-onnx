{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "torch==2.2.1\n",
        "torchvision==0.17.1\n",
        "onnx==1.16\n",
        "onnxruntime-gpu==1.17\n",
        "tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21NISVbkNLA_",
        "outputId": "a05e3564-2b41-4939-aa58-24a030e9adf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"GrobovoyDanil\"\n",
        "!git config --global user.email \"grobovoydanil@gmail.com\""
      ],
      "metadata": {
        "id": "JwQQ2C2zUH06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/GrobovoyDanil/cv-lab-pytorch-onnx.git"
      ],
      "metadata": {
        "id": "qnK3mnW6YHyS",
        "outputId": "178e8dc7-418f-4c9f-e74c-3da30e9b76c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cv-lab-pytorch-onnx'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 15 (delta 4), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 4.36 KiB | 4.36 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd cv-lab-pytorch-onnx\n"
      ],
      "metadata": {
        "id": "MMRjqWYfYTZl",
        "outputId": "8d562cb3-1dde-4f83-b331-a8b6110b90f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'cv-lab-pytorch-onnx'\n",
            "/content/cv-lab-pytorch-onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "cTtBNG_hYZ6-",
        "outputId": "9d2a9a3d-f8d2-4a50-9e4f-16f58ed85390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is behind 'origin/main' by 3 commits, and can be fast-forwarded.\n",
            "  (use \"git pull\" to update your local branch)\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git reset --hard HEAD~3\n",
        "!git push origin HEAD --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY_ZXm0hMQvK",
        "outputId": "9a6b03df-6a41-4ff8-9e68-2dedc7fd3cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: ambiguous argument 'HEAD~3': unknown revision or path not in the working tree.\n",
            "Use '--' to separate paths from revisions, like this:\n",
            "'git <command> [<revision>...] -- [<file>...]'\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb0ao8ZpKF_n"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision onnxruntime-gpu tqdm onnx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision, time, onnx, onnxruntime, numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Перевод в ГБ\n",
        "\n",
        "# gpu отчет\n",
        "import os\n",
        "markdown_content = f\"\"\"\n",
        "Имя GPU - {gpu_name}\n",
        "Объём VRAM - {gpu_memory:.2f}\n",
        "\"\"\"\n",
        "os.makedirs(\"docs\", exist_ok=True)\n",
        "\n",
        "with open(\"docs/gpu_info.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(markdown_content)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"data\", train=True,  download=True, transform=transform)\n",
        "val_ds   = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=2)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "#\n",
        "model = torchvision.models.efficientnet_b3(pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "from tqdm import tqdm   # pip install tqdm\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "def accuracy(net, loader):\n",
        "    net.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            correct += (net(x).argmax(1) == y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    return correct/total\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    # оборачиваем именно тренировочный loader\n",
        "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}\") as pbar:\n",
        "        for x, y in pbar:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(x), y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # обновляем строку прогресса текущим лоссом\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # после эпохи — метрика\n",
        "    print(f\"Epoch {epoch+1}  val-acc={accuracy(model, val_loader):.3f}\")\n",
        "\n",
        "def pytorch_inference_time(net, loader, batches=1000):\n",
        "    net.eval()\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(loader):\n",
        "            if i == batches: break\n",
        "            x = x.to(device)\n",
        "            _ = net(x)\n",
        "        torch.cuda.synchronize()\n",
        "    return time.time() - t0\n",
        "\n",
        "def onnx_inference_time(sess, loader, batches=1000):\n",
        "    input_name  = sess.get_inputs()[0].name\n",
        "    torch.cuda.synchronize() if device.type==\"cuda\" else None\n",
        "    t0 = time.time()\n",
        "    for i, (x, _) in enumerate(loader):\n",
        "        if i == batches: break\n",
        "        x = x.numpy()\n",
        "        _ = sess.run(None, {input_name: x})\n",
        "    torch.cuda.synchronize() if device.type==\"cuda\" else None\n",
        "    return time.time() - t0\n",
        "\n",
        "\n",
        "# 7а) PyTorch\n",
        "pytorch_gpu_time = pytorch_inference_time(model, val_loader)\n",
        "print(f\"PyTorch GPU: {pytorch_gpu_time:.2f} с\")\n",
        "\n",
        "# 7б) ONNX Runtime GPU\n",
        "ort_session_gpu = onnxruntime.InferenceSession(onnx_path,\n",
        "                providers=[\"CUDAExecutionProvider\"])\n",
        "onnx_gpu_time = onnx_inference_time(ort_session_gpu, val_loader)\n",
        "print(f\"ONNX  GPU:   {onnx_gpu_time:.2f} с\")\n",
        "\n",
        "# 7в) ONNX Runtime CPU\n",
        "ort_session_cpu = onnxruntime.InferenceSession(onnx_path,\n",
        "                providers=[\"CPUExecutionProvider\"])\n",
        "onnx_cpu_time = onnx_inference_time(ort_session_cpu, val_loader)\n",
        "print(f\"ONNX  CPU:   {onnx_cpu_time:.2f} с\")\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\n",
        "    \"Framework\": [\"PyTorch-GPU\", \"ONNX-GPU\", \"ONNX-CPU\"],\n",
        "    \"Time (s)\":  [pytorch_gpu_time, onnx_gpu_time, onnx_cpu_time],\n",
        "    \"Speed-up vs PT\": [1.,\n",
        "                       pytorch_gpu_time/onnx_gpu_time,\n",
        "                       pytorch_gpu_time/onnx_cpu_time]\n",
        "})\n",
        "print(df.round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "NJH6utEaOnSo",
        "outputId": "f0bb3c1c-ec01-4ccb-a5b4-d0189d3a81ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4004933738.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mval_ds\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m_check_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcheck_integrity\u001b[0;34m(fpath, md5)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcheck_md5\u001b[0;34m(fpath, md5, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcalculate_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcalculate_md5\u001b[0;34m(fpath, chunk_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mmd5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# трансформы\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
        "val_ds = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# сразу шлепнул b0, ибо не молодею\n",
        "model = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "# оптимизатор и лосс\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "def accuracy(net, loader):\n",
        "    net.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            correct += (net(x).argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "val_accuracies = []\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    # оборачиваем именно тренировочный loader\n",
        "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}\") as pbar:\n",
        "        for x, y in pbar:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(x), y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # обновляем строку прогресса текущим лоссом\n",
        "            pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # вычисление и сохранение точности\n",
        "    val_acc = accuracy(model, val_loader)\n",
        "    val_accuracies.append(val_acc)\n",
        "    print(f\"Epoch {epoch+1}  val-acc={val_acc:.3f}\")\n",
        "\n",
        "# график\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, 11), val_accuracies, marker='o')\n",
        "plt.title(\"Validation Accuracy over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.savefig(\"docs/accuracy_plot.png\")\n",
        "plt.close()\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"docs/accuracy_plot.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QoOr-Cj3fdH3",
        "outputId": "ef9b04b6-6c98-45ce-ff6e-67cc42d7a7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.0MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 159MB/s]\n",
            "Epoch 1: 100%|██████████| 782/782 [04:14<00:00,  3.07it/s, loss=0.0461]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1  val-acc=0.948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 782/782 [04:15<00:00,  3.06it/s, loss=0.0346]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2  val-acc=0.954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 782/782 [04:15<00:00,  3.07it/s, loss=0.204]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3  val-acc=0.958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 782/782 [04:14<00:00,  3.07it/s, loss=0.139]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4  val-acc=0.957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 782/782 [04:15<00:00,  3.06it/s, loss=0.487]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5  val-acc=0.954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 782/782 [04:15<00:00,  3.06it/s, loss=0.0134]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6  val-acc=0.955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 782/782 [04:15<00:00,  3.06it/s, loss=0.537]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7  val-acc=0.958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 782/782 [04:15<00:00,  3.07it/s, loss=0.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8  val-acc=0.956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 782/782 [04:15<00:00,  3.07it/s, loss=0.00471]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9  val-acc=0.959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 782/782 [04:15<00:00,  3.06it/s, loss=0.0488]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10  val-acc=0.959\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'docs/accuracy_plot.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3187545290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"docs/accuracy_plot.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;31m# savefig default implementation has no return, so mypy is unhappy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;31m# presumably this is here because subclasses can return?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[func-returns-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Need this if 'transparent=True', to reset colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m                     \u001b[0m_recursively_make_axes_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2185\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2039\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[1;32m    429\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2581\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docs/accuracy_plot.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# трансформы\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
        "val_ds = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "model_b3 = torchvision.models.efficientnet_b3(pretrained=True)\n",
        "model_b3.classifier[1] = torch.nn.Linear(model_b3.classifier[1].in_features, 10)\n",
        "model_b3 = model_b3.to(device)\n",
        "\n",
        "optimizer_b3 = torch.optim.AdamW(model_b3.parameters(), lr=3e-4)\n",
        "\n",
        "\n",
        "model_b0 = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "model_b0.classifier[1] = torch.nn.Linear(model_b0.classifier[1].in_features, 10)\n",
        "model_b0 = model_b0.to(device)\n",
        "\n",
        "optimizer_b0 = torch.optim.AdamW(model_b0.parameters(), lr=3e-4)\n",
        "\n",
        "def accuracy(net, loader):\n",
        "    net.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            correct += (net(x).argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# функция для измерения времени инференса и фпс\n",
        "def measure_fps(net, loader, batches=100):\n",
        "    net.eval()\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(loader):\n",
        "            if i == batches: break\n",
        "            x = x.to(device)\n",
        "            _ = net(x)\n",
        "    torch.cuda.synchronize()\n",
        "    total_time = time.time() - t0\n",
        "    fps = (batches * 64) / total_time\n",
        "    return total_time, fps\n",
        "\n",
        "\n",
        "# обучение b3\n",
        "model_b3.train()\n",
        "with tqdm(train_loader, desc=\"B3 Epoch 1\") as pbar:\n",
        "    for x, y in pbar:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer_b3.zero_grad()\n",
        "        loss = criterion(model_b3(x), y)\n",
        "        loss.backward()\n",
        "        optimizer_b3.step()\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "# точность и фпс для b3\n",
        "b3_acc = accuracy(model_b3, val_loader)\n",
        "b3_time, b3_fps = measure_fps(model_b3, val_loader)\n",
        "\n",
        "# обучение b0\n",
        "model_b0.train()\n",
        "with tqdm(train_loader, desc=\"B0 Epoch 1\") as pbar:\n",
        "    for x, y in pbar:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer_b0.zero_grad()\n",
        "        loss = criterion(model_b0(x), y)\n",
        "        loss.backward()\n",
        "        optimizer_b0.step()\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "# точность и фпс b0\n",
        "b0_acc = accuracy(model_b0, val_loader)\n",
        "b0_time, b0_fps = measure_fps(model_b0, val_loader)\n",
        "\n",
        "# изменения\n",
        "acc_drop = b3_acc - b0_acc\n",
        "fps_gain = b0_fps - b3_fps\n",
        "\n",
        "# создание отчета\n",
        "markdown_content = f\"\"\"\n",
        "# EfficientNet Ablation Study\n",
        "\n",
        "## Results\n",
        "- **EfficientNet-B3 Accuracy**: {b3_acc:.3f}\n",
        "- **EfficientNet-B0 Accuracy**: {b0_acc:.3f}\n",
        "- **Accuracy Drop (B3 → B0)**: {acc_drop:.3f}\n",
        "- **EfficientNet-B3 FPS**: {b3_fps:.2f}\n",
        "- **EfficientNet-B0 FPS**: {b0_fps:.2f}\n",
        "- **FPS Gain (B3 → B0)**: {fps_gain:.2f}\n",
        "\"\"\"\n",
        "os.makedirs(\"docs\", exist_ok=True)\n",
        "with open(\"docs/efficientnet_ablation.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(markdown_content)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"docs/efficientnet_ablation.md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "um0Uge4NtfLi",
        "outputId": "c71a6e49-8dce-4539-c209-b94c3b0e2393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "B3 Epoch 1: 100%|██████████| 782/782 [08:20<00:00,  1.56it/s, loss=0.11]\n",
            "B0 Epoch 1: 100%|██████████| 782/782 [04:12<00:00,  3.10it/s, loss=0.2]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b0b02f56-ea04-45a8-b9ef-4a48f538664e\", \"efficientnet_ablation.md\", 261)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# трансформы\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
        "val_ds = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "model_b3 = torchvision.models.efficientnet_b3(pretrained=True)\n",
        "model_b3.classifier[1] = torch.nn.Linear(model_b3.classifier[1].in_features, 10)\n",
        "model_b3 = model_b3.to(device)\n",
        "\n",
        "optimizer_b3 = torch.optim.AdamW(model_b3.parameters(), lr=3e-4)\n",
        "\n",
        "model_b0 = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "model_b0.classifier[1] = torch.nn.Linear(model_b0.classifier[1].in_features, 10)\n",
        "model_b0 = model_b0.to(device)\n",
        "\n",
        "optimizer_b0 = torch.optim.AdamW(model_b0.parameters(), lr=3e-4)\n",
        "\n",
        "def accuracy(net, loader):\n",
        "    net.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            correct += (net(x).argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# функция для измерения времени инференса и фпс\n",
        "def measure_fps(net, loader, batches=100):\n",
        "    net.eval()\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(loader):\n",
        "            if i == batches: break\n",
        "            x = x.to(device)\n",
        "            _ = net(x)\n",
        "    torch.cuda.synchronize()\n",
        "    total_time = time.time() - t0\n",
        "    fps = (batches * 64) / total_time\n",
        "    return total_time, fps\n",
        "\n",
        "# функция для обучения с ранней остановкой и сохранением чекпоинта\n",
        "def train_model(model, optimizer, train_loader, val_loader, model_name, patience=1, max_epochs=5):\n",
        "    best_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        with tqdm(train_loader, desc=f\"{model_name} Epoch {epoch+1}\") as pbar:\n",
        "            for x, y in pbar:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                loss = criterion(model(x), y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # вычисление валидационной точности\n",
        "        val_acc = accuracy(model, val_loader)\n",
        "        print(f\"{model_name} Epoch {epoch+1}  val-acc={val_acc:.3f}\")\n",
        "\n",
        "        # сохранение чекпоинта при улучшении точности\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            checkpoint = {\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch + 1,\n",
        "                'val_acc': val_acc\n",
        "            }\n",
        "            from google.colab import files\n",
        "            os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "            torch.save(checkpoint, f\"checkpoints/{model_name}_checkpoint.tar\")\n",
        "            files.download(f\"checkpoints/{model_name}_checkpoint.tar\")\n",
        "            print(f\"Saved checkpoint for {model_name} at epoch {epoch+1}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # стопаем\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping for {model_name}, epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    return val_acc\n",
        "\n",
        "# обучение b3 с ранней остановкой\n",
        "b3_acc = train_model(model_b3, optimizer_b3, train_loader, val_loader, \"EfficientNet-B3\")\n",
        "\n",
        "# точность и фпс для b3\n",
        "b3_time, b3_fps = measure_fps(model_b3, val_loader)\n",
        "\n",
        "# обучение b0 с ранней остановкой\n",
        "b0_acc = train_model(model_b0, optimizer_b0, train_loader, val_loader, \"EfficientNet-B0\")\n",
        "\n",
        "# точность и фпс b0\n",
        "b0_time, b0_fps = measure_fps(model_b0, val_loader)\n",
        "\n",
        "# изменения\n",
        "acc_drop = b3_acc - b0_acc\n",
        "fps_gain = b0_fps - b3_fps\n",
        "\n",
        "# создание отчета\n",
        "markdown_content = f\"\"\"\n",
        "# EfficientNet Ablation Study\n",
        "\n",
        "## Results\n",
        "- **EfficientNet-B3 Accuracy**: {b3_acc:.3f}\n",
        "- **EfficientNet-B0 Accuracy**: {b0_acc:.3f}\n",
        "- **Accuracy Drop (B3 → B0)**: {acc_drop:.3f}\n",
        "- **EfficientNet-B3 FPS**: {b3_fps:.2f}\n",
        "- **EfficientNet-B0 FPS**: {b0_fps:.2f}\n",
        "- **FPS Gain (B3 → B0)**: {fps_gain:.2f}\n",
        "\"\"\"\n",
        "os.makedirs(\"docs\", exist_ok=True)\n",
        "with open(\"docs/efficientnet_ablation_with_break.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(markdown_content)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"docs/efficientnet_ablation_with_break.md\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "QYXRcpNpxqt9",
        "outputId": "f09acc24-6a3f-4f6f-95e8-10aa0a7f070d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EfficientNet-B3 Epoch 1: 100%|██████████| 782/782 [08:21<00:00,  1.56it/s, loss=0.375]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientNet-B3 Epoch 1  val-acc=0.961\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1f7778db-eb12-42cf-b084-fb975dc3662f\", \"EfficientNet-B3_checkpoint.tar\", 129410461)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint for EfficientNet-B3 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EfficientNet-B3 Epoch 2: 100%|██████████| 782/782 [08:20<00:00,  1.56it/s, loss=0.0272]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientNet-B3 Epoch 2  val-acc=0.968\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ce671dd6-97d5-49bf-a8a4-8f32ca6b3736\", \"EfficientNet-B3_checkpoint.tar\", 129410461)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint for EfficientNet-B3 at epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EfficientNet-B3 Epoch 3: 100%|██████████| 782/782 [08:20<00:00,  1.56it/s, loss=0.0293]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientNet-B3 Epoch 3  val-acc=0.967\n",
            "Early stopping for EfficientNet-B3, epoch 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EfficientNet-B0 Epoch 1: 100%|██████████| 782/782 [04:12<00:00,  3.10it/s, loss=0.267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientNet-B0 Epoch 1  val-acc=0.945\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e57ba8f5-628f-44a5-9e18-a0133f512752\", \"EfficientNet-B0_checkpoint.tar\", 48741256)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint for EfficientNet-B0 at epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EfficientNet-B0 Epoch 2: 100%|██████████| 782/782 [04:11<00:00,  3.11it/s, loss=0.0886]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientNet-B0 Epoch 2  val-acc=0.957\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a78a239f-7cdc-436d-9d9b-deeabc8588ab\", \"EfficientNet-B0_checkpoint.tar\", 48741256)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint for EfficientNet-B0 at epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "EfficientNet-B0 Epoch 3: 100%|██████████| 782/782 [04:12<00:00,  3.10it/s, loss=0.577]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientNet-B0 Epoch 3  val-acc=0.954\n",
            "Early stopping for EfficientNet-B0, epoch 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: efficientnet_ablation_with_break.md",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3328283406.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;31m# from google.colab import files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"efficientnet_ablation_with_break.md\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: efficientnet_ablation_with_break.md"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# трансформы\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_ds = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
        "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)  # изменил batch_size на 1 для обработки одной картинки за раз\n",
        "\n",
        "model_b3 = torchvision.models.efficientnet_b3(pretrained=True)\n",
        "model_b3.classifier[1] = torch.nn.Linear(model_b3.classifier[1].in_features, 10)\n",
        "model_b3 = model_b3.to(device)\n",
        "\n",
        "model_b0 = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "model_b0.classifier[1] = torch.nn.Linear(model_b0.classifier[1].in_features, 10)\n",
        "model_b0 = model_b0.to(device)\n",
        "\n",
        "# задержка одной картинки\n",
        "def measure_latency(model, loader, runs=100):\n",
        "    model.eval()\n",
        "    latencies = []\n",
        "    torch.cuda.synchronize()\n",
        "    for _ in range(runs):\n",
        "        x, _ = next(iter(loader))  # берем одну картинку из загрузчика\n",
        "        x = x.to(device)\n",
        "        t0 = time.time()\n",
        "        with torch.no_grad():\n",
        "            _ = model(x)\n",
        "        torch.cuda.synchronize()\n",
        "        latencies.append(time.time() - t0)\n",
        "    return sum(latencies) / len(latencies)  # средняя задержка\n",
        "\n",
        "b3_latency = measure_latency(model_b3, val_loader)\n",
        "b0_latency = measure_latency(model_b0, val_loader)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "models = [\"EfficientNet-B3\", \"EfficientNet-B0\"]\n",
        "latencies = [b3_latency * 1000, b0_latency * 1000]  # перевод в миллисекунды для восприятия\n",
        "plt.bar(models, latencies, color=['#1f77b4', '#ff7f0e'])\n",
        "plt.title(\"Single Image Inference Latency (ms)\")\n",
        "plt.ylabel(\"Latency (ms)\")\n",
        "plt.grid(True, axis='y')\n",
        "os.makedirs(\"docs\", exist_ok=True)\n",
        "plt.savefig(\"docs/latency.png\")\n",
        "plt.close()\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"docs/latency.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "4ZM4Vy2dbQYc",
        "outputId": "7ff3d95f-ad4e-4381-86bb-2955ff87e26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.6MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 47.2M/47.2M [00:00<00:00, 216MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 236MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_194b56c3-4d0a-4fd0-95aa-f86b0f27df76\", \"latency.png\", 16725)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision onnxruntime-gpu tqdm onnx torch-tensorrt\n",
        "# !sudo apt install tensorrt"
      ],
      "metadata": {
        "id": "U2auOeXcHxzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233fdf9e-01e0-4d56-ffef-026d01be59d8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.5/300.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorrt-cu12-libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Подключите TensorrtExecutionProvider, измерьте throughput. Скриншот вывода в PR.\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import os\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_ds = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "# экспорт модели в ONNX\n",
        "def export_to_onnx(model, model_name):\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        f\"checkpoints/{model_name}.onnx\",\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"output\"],\n",
        "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        "        opset_version=11\n",
        "    )\n",
        "    print(f\"Exported {model_name} to ONNX\")\n",
        "\n",
        "# throughput\n",
        "def measure_tensorrt_throughput(model_name, loader, batches=100):\n",
        "    session = ort.InferenceSession(\n",
        "        f\"checkpoints/{model_name}.onnx\",\n",
        "        providers=[\"TensorrtExecutionProvider\"]\n",
        "    )\n",
        "    input_name = session.get_inputs()[0].name\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    for i, (x, _) in enumerate(loader):\n",
        "        if i == batches: break\n",
        "        x = x.numpy()  # Конвертация в numpy для ONNX\n",
        "        _ = session.run(None, {input_name: x})\n",
        "    torch.cuda.synchronize()\n",
        "    total_time = time.time() - t0\n",
        "    throughput = (batches * 64) / total_time  # Изображений в секунду\n",
        "    return throughput\n",
        "\n",
        "# Экспорт моделей в ONNX\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "export_to_onnx(model, \"EfficientNet-B0\")\n",
        "\n",
        "# Измерение throughput для B3 и B0\n",
        "throughput = measure_tensorrt_throughput(\"EfficientNet-B0\", val_loader)\n",
        "\n",
        "print(f\"throughput {throughput}\")\n",
        "\n",
        "import onnxruntime as ort\n",
        "providers = ort.get_available_providers()\n",
        "print(providers)"
      ],
      "metadata": {
        "id": "oXxXegMvTy2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b1eb07-9fe1-4b5f-c6f4-cc98fa7247ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.5MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 121MB/s] \n",
            "/tmp/ipython-input-3820346221.py:34: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported EfficientNet-B0 to ONNX\n",
            "*************** EP Error ***************\n",
            "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:559 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
            " when using ['TensorrtExecutionProvider']\n",
            "Falling back to ['CPUExecutionProvider'] and retrying.\n",
            "****************************************\n",
            "throughput 27.239904819985785\n",
            "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Постройте график speed-up PyTorch vs ONNX vs INT8. Сохраните docs/speedup.png.\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import os\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# трансформы\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_ds = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "# экспорт модели в ONNX\n",
        "def export_to_onnx(model, model_name):\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        f\"checkpoints/{model_name}.onnx\",\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"output\"],\n",
        "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        "        opset_version=11\n",
        "    )\n",
        "    print(f\"Exported {model_name} to ONNX\")\n",
        "\n",
        "# квантование модели в INT8\n",
        "def quantize_model_to_int8(input_path, output_path):\n",
        "    model = onnx.load(input_path)\n",
        "    quantized_model = quantize_dynamic(input_path, output_path, weight_type=QuantType.QInt8)\n",
        "    print(f\"Quantized model saved to {output_path}\")\n",
        "\n",
        "def pytorch_inference_time(net, loader, batches=1000):\n",
        "    net.eval()\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (x, _) in enumerate(loader):\n",
        "            if i == batches: break\n",
        "            x = x.to(device)\n",
        "            _ = net(x)\n",
        "        torch.cuda.synchronize()\n",
        "    return time.time() - t0\n",
        "\n",
        "def onnx_inference_time(sess, loader, batches=1000):\n",
        "    input_name = sess.get_inputs()[0].name\n",
        "    torch.cuda.synchronize() if device.type==\"cuda\" else None\n",
        "    t0 = time.time()\n",
        "    for i, (x, _) in enumerate(loader):\n",
        "        if i == batches: break\n",
        "        x = x.numpy()\n",
        "        _ = sess.run(None, {input_name: x})\n",
        "    torch.cuda.synchronize() if device.type==\"cuda\" else None\n",
        "    return time.time() - t0\n",
        "\n",
        "# Экспорт модели в ONNX\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "export_to_onnx(model, \"EfficientNet-B0\")\n",
        "\n",
        "# Квантование модели в INT8\n",
        "# quantize_model_to_int8(\"checkpoints/EfficientNet-B0.onnx\", \"checkpoints/EfficientNet-B0_int8.onnx\")\n",
        "\n",
        "# 7а) PyTorch\n",
        "pytorch_gpu_time = pytorch_inference_time(model, val_loader)\n",
        "print(f\"PyTorch GPU: {pytorch_gpu_time:.2f} с\")\n",
        "\n",
        "# 7б) ONNX Runtime GPU\n",
        "ort_session_gpu = ort.InferenceSession(\"checkpoints/EfficientNet-B0.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
        "onnx_gpu_time = onnx_inference_time(ort_session_gpu, val_loader)\n",
        "print(f\"ONNX  GPU:   {onnx_gpu_time:.2f} с\")\n",
        "\n",
        "# 7в) ONNX Runtime CPU\n",
        "ort_session_cpu = ort.InferenceSession(\"checkpoints/EfficientNet-B0.onnx\", providers=[\"CPUExecutionProvider\"])\n",
        "onnx_cpu_time = onnx_inference_time(ort_session_cpu, val_loader)\n",
        "print(f\"ONNX  CPU:   {onnx_cpu_time:.2f} с\")\n",
        "\n",
        "# # NT8\n",
        "# ort_session_int8 = ort.InferenceSession(\"checkpoints/EfficientNet-B0_int8.onnx\", providers=[\"CUDAExecutionProvider\"])\n",
        "# onnx_int8_time = onnx_inference_time(ort_session_int8, val_loader)\n",
        "# print(f\"ONNX INT8:   {onnx_int8_time:.2f} с\")\n",
        "\n",
        "quant_path = \"effnet_b3_cifar10_int8.onnx\"\n",
        "quantize_dynamic(\"checkpoints/EfficientNet-B0.onnx\", quant_path, weight_type=QuantType.QUInt8)\n",
        "\n",
        "# Прогоняем ещё раз\n",
        "ort_session_int8 = ort.InferenceSession(quant_path,\n",
        "                providers=[\"CPUExecutionProvider\"])\n",
        "onnx_int8_time = onnx_inference_time(ort_session_int8, val_loader)\n",
        "print(f\"ONNX-CPU-INT8: {onnx_int8_time:.2f} с\")\n",
        "\n",
        "# speed-up\n",
        "speedup_gpu = pytorch_gpu_time / onnx_gpu_time\n",
        "speedup_cpu = pytorch_gpu_time / onnx_cpu_time\n",
        "speedup_int8 = pytorch_gpu_time / onnx_int8_time\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "frameworks = [\"PyTorch-GPU\", \"ONNX-GPU\", \"ONNX-CPU\", \"ONNX-INT8\"]\n",
        "speedups = [1.0, speedup_gpu, speedup_cpu, speedup_int8]\n",
        "plt.bar(frameworks, speedups, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "plt.title(\"Speed-up vs PyTorch-GPU\")\n",
        "plt.ylabel(\"Speed-up\")\n",
        "plt.grid(True, axis='y')\n",
        "os.makedirs(\"docs\", exist_ok=True)\n",
        "plt.savefig(\"docs/speedup.png\")\n",
        "plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvKeSWdpLdU6",
        "outputId": "c9faafc8-28fc-43d9-88cb-b0177348fc10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3714165593.py:36: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported EfficientNet-B0 to ONNX\n",
            "PyTorch GPU: 18.24 с\n",
            "ONNX  GPU:   21.70 с\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX  CPU:   334.63 с\n",
            "ONNX-CPU-INT8: 666.05 с\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import os\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# трансформы\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_ds = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)\n",
        "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "# Экспорт модели в ONNX\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "model.eval()\n",
        "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"checkpoints/EfficientNet-B0.onnx\",\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        "    opset_version=11\n",
        ")\n",
        "print(\"Exported EfficientNet-B0 to ONNX\")\n",
        "\n",
        "# квантование модели в INT8\n",
        "quant_path = \"checkpoints/EfficientNet-B0_int8.onnx\"\n",
        "quantize_dynamic(\"checkpoints/EfficientNet-B0.onnx\", quant_path, weight_type=QuantType.QUInt8)\n",
        "print(f\"Quantized model saved to {quant_path}\")\n",
        "\n",
        "# Функция вычисления Top-1 accuracy\n",
        "def evaluate_onnx_top1(sess, loader, max_batches=16):  # 16*64≈1024 картинок\n",
        "    input_name = sess.get_inputs()[0].name\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        if i == max_batches:\n",
        "            break\n",
        "        x = x.numpy()\n",
        "        y = y.numpy()\n",
        "        outputs = sess.run(None, {input_name: x})[0]\n",
        "        preds = outputs.argmax(axis=1)\n",
        "        correct += (preds == y).sum()\n",
        "        total += y.shape[0]\n",
        "    return correct / total\n",
        "\n",
        "# Оценка float32\n",
        "ort_sess_float = ort.InferenceSession(\"checkpoints/EfficientNet-B0.onnx\",\n",
        "                                     providers=[\"CPUExecutionProvider\"])\n",
        "top1_float = evaluate_onnx_top1(ort_sess_float, val_loader)\n",
        "print(f\"Top-1 accuracy float32: {top1_float:.4f}\")\n",
        "\n",
        "# Оценка INT8\n",
        "ort_sess_int8 = ort.InferenceSession(quant_path, providers=[\"CPUExecutionProvider\"])\n",
        "top1_int8 = evaluate_onnx_top1(ort_sess_int8, val_loader)\n",
        "print(f\"Top-1 accuracy INT8: {top1_int8:.4f}\")\n",
        "\n",
        "# Сохраняем вывод в файл\n",
        "os.makedirs(\"docs\", exist_ok=True)\n",
        "with open(\"docs/quantization.md\", \"w\") as f:\n",
        "    f.write(f\"# Top-1 Accuracy Comparison\\n\\n\")\n",
        "    f.write(f\"- Float32: {top1_float:.4f}\\n\")\n",
        "    f.write(f\"- INT8: {top1_int8:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg4L4l1t4MiL",
        "outputId": "369b602a-544d-4c25-ac15-13fb87d36931"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-661556260.py:32: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n",
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported EfficientNet-B0 to ONNX\n",
            "Quantized model saved to checkpoints/EfficientNet-B0_int8.onnx\n",
            "Top-1 accuracy float32: 0.0537\n",
            "Top-1 accuracy INT8: 0.1279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"GrobovoyDanil\"\n",
        "!git config --global user.email \"grobovoydanil@gmail.com\""
      ],
      "metadata": {
        "id": "3asGf4dcSFrn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/GrobovoyDanil/cv-lab-pytorch-onnx.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERYtUZcDSg-E",
        "outputId": "ce6b0a5a-a33a-4f44-a421-110fc6bce7a0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cv-lab-pytorch-onnx' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r docs checkpoints cv-lab-pytorch-onnx/\n",
        "# !cp effnet_b3_cifar10_int8.onnx cv-lab-pytorch-onnx/\n",
        "!cp Лаб_3_Нейронки.ipynb cv-lab-pytorch-onnx/"
      ],
      "metadata": {
        "id": "iuKVfXBbSoon"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd cv-lab-pytorch-onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08X7s3-PS-Nm",
        "outputId": "8a089fcd-4c3c-4cd8-c5b4-6db6dfbcdee9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cv-lab-pytorch-onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"added some stuff for lab(docs, checkpoints) and updated code\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrKwJH1LUSEP",
        "outputId": "5eb24497-4621-4355-d565-0e7cfde16ba0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main d986499] added some stuff for lab(docs, checkpoints) and updated code\n",
            " 1 file changed, 2208 insertions(+), 19 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKRzsvmCUpEx",
        "outputId": "28aab5f9-98e2-4cd3-9c22-39908a00bbbf"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 5, done.\n",
            "Counting objects:  20% (1/5)\rCounting objects:  40% (2/5)\rCounting objects:  60% (3/5)\rCounting objects:  80% (4/5)\rCounting objects: 100% (5/5)\rCounting objects: 100% (5/5), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects:  33% (1/3)\rCompressing objects:  66% (2/3)\rCompressing objects: 100% (3/3)\rCompressing objects: 100% (3/3), done.\n",
            "Writing objects:  33% (1/3)\rWriting objects:  66% (2/3)\rWriting objects: 100% (3/3)\rWriting objects: 100% (3/3), 50.66 KiB | 10.13 MiB/s, done.\n",
            "Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "remote: \u001b[1;31merror\u001b[m: GH013: Repository rule violations found for refs/heads/main.\u001b[K\n",
            "remote: \n",
            "remote: - GITHUB PUSH PROTECTION\u001b[K\n",
            "remote:   —————————————————————————————————————————\u001b[K\n",
            "remote:     Resolve the following violations before pushing again\u001b[K\n",
            "remote: \n",
            "remote:     - Push cannot contain secrets\u001b[K\n",
            "remote: \n",
            "remote:     \u001b[K\n",
            "remote:      (?) Learn how to resolve a blocked push\u001b[K\n",
            "remote:      https://docs.github.com/code-security/secret-scanning/working-with-secret-scanning-and-push-protection/working-with-push-protection-from-the-command-line#resolving-a-blocked-push\u001b[K\n",
            "remote:     \u001b[K\n",
            "remote:     \u001b[K\n",
            "remote:       —— GitHub Personal Access Token ——————————————————————\u001b[K\n",
            "remote:        locations:\u001b[K\n",
            "remote:          - commit: d986499e4408732bee83245a917607de1f99cc9a\u001b[K\n",
            "remote:            path: Лаб_3_Нейронки.ipynb:171\u001b[K\n",
            "remote:          - commit: d986499e4408732bee83245a917607de1f99cc9a\u001b[K\n",
            "remote:            path: Лаб_3_Нейронки.ipynb:2211\u001b[K\n",
            "remote:     \u001b[K\n",
            "remote:        (?) To push, remove secret from commit(s) or follow this URL to allow the secret.\u001b[K\n",
            "remote:        https://github.com/GrobovoyDanil/cv-lab-pytorch-onnx/security/secret-scanning/unblock-secret/343x5CvTAmxnDRmVileyJWhyniZ\u001b[K\n",
            "remote:     \u001b[K\n",
            "remote: \n",
            "remote: \n",
            "To https://github.com/GrobovoyDanil/cv-lab-pytorch-onnx\n",
            " \u001b[31m! [remote rejected]\u001b[m HEAD -> main (push declined due to repository rule violations)\n",
            "\u001b[31merror: failed to push some refs to 'https://github.com/GrobovoyDanil/cv-lab-pytorch-onnx'\n",
            "\u001b[m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git reset --soft HEAD~1"
      ],
      "metadata": {
        "id": "BrELYUPDbS51"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}